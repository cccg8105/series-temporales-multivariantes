---
title: "Crecimiento salarial"
author: "César Cárdenas"
date: "8/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("env_rstudio")
```

## Descripción

## Librerias

```{python}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/cesar.cardenas/anaconda3/envs/env_rstudio/Library/plugins/platforms'

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import statsmodels.tsa.stattools as sts
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import grangercausalitytests
import statsmodels.graphics.tsaplots as sgt
from sklearn.metrics import mean_absolute_error, mean_squared_error

import warnings
warnings.simplefilter('ignore')
```

## Importación de datos

```{python}
df_comp = pd.read_csv('https://raw.githubusercontent.com/cccg8105/series-temporales-multivariantes/master/3.%20Casos%20de%20estudio%20en%20Python/C.%20Crecimiento%20salarial%20y%20el%20proceso%20de%20inflaci%C3%B3n/Raotbl6.csv', parse_dates=['date'], index_col='date')
print(df_comp.shape)  # (123, 8)
df_comp.tail()
```

```{python}
df_comp.isnull().sum()
```

## Analisis exploratorio

```{python}
# Plot
fig, axes = plt.subplots(nrows=4, ncols=2, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    data = df_comp[df_comp.columns[i]]
    ax.plot(data, color='red', linewidth=1)
    # Decorations
    ax.set_title(df_comp.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout();
plt.show()
```

Matriz de correlación

```{python}
corr=df_comp.corr()
corr
```

```{python}
sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True,vmax=1, vmin=-1, cmap =sns.diverging_palette(220, 10, as_cmap=True),center=0 )
plt.show()
```

## División de datos en entrenamiento y prueba

```{python}
n_obs=15
X_train, X_test = df_comp[0:-n_obs], df_comp[-n_obs:]
print(X_train.shape, X_test.shape)
```

## Prueba de estacionariedad

```{python}
def augmented_dickey_fuller_statistics(time_series):
  result = sts.adfuller(time_series.values)
  print('p-value: %f' % result[1])

print('Test de Dickey-Fuller Aumentado:')
for i in range(8):
  print('Serie de tiempo',i+1)
  augmented_dickey_fuller_statistics(X_train.iloc[:,i])

```

Todos los p-valores son mayores que 0.05, con lo cual no se rechaza la hipótesis nula de que la serie no es estacionaria. Habría que transformar los datos porque no se puede confirmar estacionariedad.

## Transformación de datos

```{python}
X_train_transformed=X_train.diff().dropna()
X_train_transformed.head()
```

```{python}

# Plot
fig, axes = plt.subplots(nrows=4, ncols=2, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    data = X_train_transformed[X_train_transformed.columns[i]]
    ax.plot(data, color='red', linewidth=1)
    # Decorations
    ax.set_title(X_train_transformed.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)

plt.tight_layout();
plt.show()
```

Se realiza la prueba nuevamente para verificar si son estacionarios

```{python}
print('Test de Dickey-Fuller Aumentado 2da vez:')
for i in range(8):
  print('Serie de tiempo transformada', 'Variable',i+1)
  augmented_dickey_fuller_statistics(X_train_transformed.iloc[:,i])

```

La mayoría aún no son estacionarias y debe diferenciarse por 2da vez

```{python}
#Diferenciando por segunda vez
X_train_transformed=X_train_transformed.diff().dropna()
X_train_transformed.head()
```

```{python}
print('Test de Dickey-Fuller Aumentado 3ra vez:')
for i in range(8):
  print('Serie de tiempo transformada', 'Variable',i+1)
  augmented_dickey_fuller_statistics(X_train_transformed.iloc[:,i])

```

## Modelo VAR

```{python}
model = VAR(X_train_transformed)

modelsel=model.select_order(10)
print(modelsel.summary())
```

## Ajustando el modelo

```{python}
res = model.fit(maxlags=10, ic='aic') 
print(res.summary())
```

```{python}
X_train.columns
```

## Causalidad de Granger

### rgnp

```{python}
grangercaus=res.test_causality(['pgnp', 'ulc', 'gdfco', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['rgnp'],kind='f')
print(grangercaus.summary())

```

### pgnp

```{python}
grangercaus=res.test_causality(['rgnp', 'ulc', 'gdfco', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['pgnp'],kind='f')
print(grangercaus.summary())
```

### ulc

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'gdfco', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['ulc'],kind='f')
print(grangercaus.summary())
```

### gdfco

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['gdfco'],kind='f')
print(grangercaus.summary())
```

El p valor es mas alto que 0.05.

### gdf

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdfco', 'gdfim', 'gdfcf', 'gdfce'],['gdf'],kind='f')
print(grangercaus.summary())
```

### gdfim

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdfco', 'gdf', 'gdfcf', 'gdfce'],['gdfim'],kind='f')
print(grangercaus.summary())
```

### gdfcf

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdfco', 'gdf', 'gdfim', 'gdfce'],['gdfcf'],kind='f')
print(grangercaus.summary())
```

### gdfce

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdfco', 'gdf', 'gdfim', 'gdfcf'],['gdfce'],kind='f')
print(grangercaus.summary())
```

Conclusión: gdfco problemática

Matriz de causalidad de Granger

```{python}
maxlag=15
test = 'ssr_chi2test'
def grangers_causality_matrix(X_train_transformed, variables, test = 'ssr_chi2test', verbose=False):
  dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
  for c in dataset.columns:
    for r in dataset.index:
      test_result = grangercausalitytests(X_train_transformed[[r,c]], maxlag=maxlag, verbose=False)
      p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
      if verbose: 
        print(f'Y = {r}, X = {c}, P Values = {p_values}')
      min_p_value = np.min(p_values)
      dataset.loc[r,c] = min_p_value
  dataset.columns = [var + '_x' for var in variables]
  dataset.index = [var + '_y' for var in variables]
  return dataset
grangers_causality_matrix(X_train_transformed, variables = X_train_transformed.columns)
```
gdfco no causa la mayoría de las otras variables y tampoco es causada por algunas de ellas.

## Diagnosis del modelo

```{python}
residuals=res.resid
fig, axs = plt.subplots(8)
fig.suptitle('Gráficos de los residuos',fontsize=20)
fig.set_size_inches(18, 10)
[axs[i].plot(residuals.iloc[:,i]) for i in range(8)]
plt.show()
```

Prueba de estacionariedad en residuos

```{python}
print('ADF de los Residuos, p-valores:')
[sts.adfuller(residuals.iloc[:,i])[1] for i in range(8)]
```

Autocorrelacion en residuos

```{python}
# [sgt.plot_acf(residuals.iloc[:,i], zero = False, lags = 40) for i in range(8)]

for i in range(8):
  sgt.plot_acf(residuals.iloc[:,i], zero = False, lags = 40)
  plt.show()

```

Conclusión: Los residuos del modelo no presentan estructura de autocorrelación, son estacionarios según los resultados de la prueba de Dickey - Fuller aumentada y en los gráficos se puede comprobar esto visualmente, entonces puede concluirse que son ruido blanco como es deseable.

Valores predichos por el modelo

```{python}
y_fitted = res.fittedvalues
fig, axs = plt.subplots(8)
fig.suptitle('Gráficos de los valores predichos por el modelo',fontsize=20)
fig.set_size_inches(18, 10)
[axs[i].plot(y_fitted.iloc[:,i]) for i in range(8)]
plt.show()
```

## Predicción

```{python}
# Obtener el orden del modelo
lag_order = res.k_ar
print('Orden del modelo:', lag_order)
# Input data para hacer forecasting (pronósticos a futuro)
input_data = X_train_transformed.values[-lag_order:]
# Forecasting
pred = res.forecast(y=input_data, steps=n_obs)
pred = (pd.DataFrame(pred, index=X_test.index, columns=X_test.columns + '_pred'))
print('Predicciones:')
pred
```
```{python}
plt.figure(figsize = (12, 10))
res.plot_forecast(lag_order)
plt.tight_layout(h_pad = 1.15)
plt.show()
```

Invertir transformación

```{python}
# Invirtiendo la transformación teniendo en cuenta que hemos diferenciado dos veces

def invert_transformation(ds, df_forecast, second_diff=False):
  for col in ds.columns:
    # Undo the 2nd Differencing
    if second_diff:
      df_forecast[str(col)] = (ds[col].iloc[-1] - ds[col].iloc[-2]) + df_forecast[str(col)].cumsum()
      # Undo the 1st Differencing
      df_forecast[str(col)] = ds[col].iloc[-1] + df_forecast[str(col)].cumsum()
  return df_forecast

```

```{python}
pred.columns=X_test.columns

output = invert_transformation(X_train, pred, second_diff=True)
print(output)

```

Comparación de resultados con los datos de prueba

```{python}
# Actual vs Forecasted Plots
fig, axes = plt.subplots(nrows = int(len(X_train.columns)/2), ncols = 2, dpi = 100, figsize = (10,10))

for i, (col,ax) in enumerate(zip(X_train.columns, axes.flatten())):
    output[col].plot(color = '#F4511E', legend = True, ax = ax).autoscale(axis =' x',tight = True)
    X_test[col].plot(color = '#3949AB', legend = True, ax = ax)

    ax.set_title('Variable: ' + col + ' - Actual vs Forecast')
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')

    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize = 6)

plt.tight_layout()
plt.savefig('actual_forecast.png')
plt.show()
```

### Evaluación del modelo

```{python}

print('Mean absolute error:', mean_absolute_error(X_test, output))
print('Root mean squared error:', np.sqrt(mean_squared_error(X_test, output)))
```

### Eliminando variable no causal

```{python}
X_train_transformed_new=X_train_transformed.drop(['gdfco'],axis=1)
X_train_transformed_new.head()
```

### Nuevo modelo VAR

```{python}
model_new = VAR(X_train_transformed_new)

modelsel_new=model_new.select_order(10)
print(modelsel_new.summary())
```
Ajustando el modelo

```{python}
res_new = model_new.fit(maxlags=10, ic='aic')
print(res_new.summary())
```

### Causalidad de granger

#### rgnp

```{python}
grangercaus=res_new.test_causality(['pgnp', 'ulc', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['rgnp'],kind='f')
print(grangercaus.summary())
```
#### pgnp

```{python}
grangercaus=res_new.test_causality(['rgnp', 'ulc', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['pgnp'],kind='f')
print(grangercaus.summary())
```

#### ulc

```{python}
grangercaus=res_new.test_causality(['rgnp', 'pgnp', 'gdf', 'gdfim', 'gdfcf', 'gdfce'],['ulc'],kind='f')
print(grangercaus.summary())
```

#### gdf

```{python}
grangercaus=res_new.test_causality(['rgnp', 'pgnp', 'ulc', 'gdfim', 'gdfcf', 'gdfce'],['gdf'],kind='f')
print(grangercaus.summary())
```

#### gdfim

```{python}
grangercaus=res.test_causality(['rgnp', 'pgnp', 'ulc', 'gdf', 'gdfcf', 'gdfce'],['gdfim'],kind='f')
print(grangercaus.summary())
```

#### gdfcf

```{python}
grangercaus=res_new.test_causality(['rgnp', 'pgnp', 'ulc', 'gdf', 'gdfim', 'gdfce'],['gdfcf'],kind='f')
print(grangercaus.summary())
```

#### gdfce

```{python}
grangercaus=res_new.test_causality(['rgnp', 'pgnp', 'ulc', 'gdf', 'gdfim', 'gdfcf'],['gdfce'],kind='f')
print(grangercaus.summary())
```

Matriz de causalidad 1 vs 1

```{python}
warnings.filterwarnings("ignore")
maxlag=10
test = 'ssr_chi2test'
def grangers_causality_matrix(X_train_transformed, variables, test = 'ssr_chi2test', verbose=False):
  dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
  for c in dataset.columns:
    for r in dataset.index:
      test_result = grangercausalitytests(X_train_transformed[[r,c]], maxlag=maxlag, verbose=False)
      p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
      if verbose: 
        print(f'Y = {r}, X = {c}, P Values = {p_values}')
      min_p_value = np.min(p_values)
      dataset.loc[r,c] = min_p_value
  dataset.columns = [var + '_x' for var in variables]
  dataset.index = [var + '_y' for var in variables]
  return dataset
grangers_causality_matrix(X_train_transformed_new, variables = X_train_transformed_new.columns)
```

### Nuevos pronosticos

se elimina columna del conjunto de datos de prueba

```{python}
X_test_new=X_test.drop(['gdfco'],axis=1)
X_test_new.head()
```

Obteniendo las predicciones

```{python}
# Obtener el orden del modelo
lag_order = res_new.k_ar
print('Orden del modelo:', lag_order)
# Input data para hacer forecasting (pronósticos a futuro)
input_data = X_train_transformed_new.values[-lag_order:]
# Forecasting
pred_new = res_new.forecast(y=input_data, steps=n_obs)
pred_new = (pd.DataFrame(pred_new, index=X_test_new.index, columns=X_test_new.columns + '_pred'))
print('Predicciones:')
pred_new
```

Invirtiendo la transformación

```{python}
X_train_new=X_train.drop(['gdfco'],axis=1)

pred_new.columns=X_test_new.columns
output = invert_transformation(X_train_new, pred_new, second_diff=True)
output
```

Comparación de resultados con datos de prueba

```{python}

# Actual vs Forecasted Plots
fig, axes = plt.subplots(nrows = int(len(X_train_new.columns)/2), ncols = 2, dpi = 100, figsize = (10,10))

for i, (col,ax) in enumerate(zip(X_train_new.columns, axes.flatten())):
    output[col].plot(color = '#F4511E', legend = True, ax = ax).autoscale(axis =' x',tight = True)
    X_test_new[col].plot(color = '#3949AB', legend = True, ax = ax)

    ax.set_title('Variable: ' + col + ' - Actual vs Forecast')
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')

    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize = 6)

plt.tight_layout()
plt.savefig('actual_forecast.png')
plt.show()
```

### Evaluación del modelo

```{python}
print('Mean absolute error:', mean_absolute_error(X_test_new, output))
print('Root mean squared error:', np.sqrt(mean_squared_error(X_test_new, output)))
```

Conclusión: Los resultados anteriores fueron:
Mean absolute error: 24.1862
Root mean squared error: 51.2649
En este caso ha mejorado el rendimiento del modelo al eliminar una variable redundante: 'gdfco'.